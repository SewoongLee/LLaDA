{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from generate import *\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "model = AutoModel.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True, torch_dtype=torch.bfloat16).to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True)\n",
    "\n",
    "prompt = \"Lily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour. How many kilometers can she run in 8 hours?\"\n",
    "\n",
    "# Add special tokens for the Instruct model. The Base model does not require the following two lines.\n",
    "m = [{\"role\": \"user\", \"content\": prompt}, ]\n",
    "prompt = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "input_ids = tokenizer(prompt)['input_ids']\n",
    "input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "\n",
    "out = generate(model, input_ids, steps=128, gen_length=128, block_length=32, temperature=0., cfg_scale=0., remasking='low_confidence')\n",
    "print(tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True)[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
